{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ff2334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "SEED = 45\n",
    "np.random.seed(SEED)\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from models import GibbsSamplerLLFM\n",
    "from evals import latent_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eff59ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities P_true: [[0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.5        0.00247262 0.5       ]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.00247262 0.00247262 0.00247262 0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]\n",
      " [0.5        0.00247262 0.5        0.00247262]]\n"
     ]
    }
   ],
   "source": [
    "def generate_synthetic(T=150, S=4, K_true=2):\n",
    "\n",
    "    \n",
    "\n",
    "    # ----- True latent features -----\n",
    "    Z_true = np.zeros((T, K_true))\n",
    "    Z_true[:70, 0] = 1\n",
    "    Z_true[110:, 1] = 1\n",
    "    #Z_true[50:75, :] = 1\n",
    "    \n",
    "\n",
    "    # ----- True weights -----\n",
    "    W_true = np.zeros((K_true, S))\n",
    "    W_true[0,1] = 6\n",
    "    W_true[0,3] = 6\n",
    "    W_true[1,0] = 6\n",
    "    W_true[1,2] = 6\n",
    "\n",
    "\n",
    "    # ----- True bias -----\n",
    "    b_true = np.array([-6, -6, -6, -6])\n",
    "\n",
    "    # ----- Generate observations -----\n",
    "    logits = Z_true @ W_true + b_true\n",
    "    P_true = expit(logits)\n",
    "    Y = np.random.binomial(1, P_true)\n",
    "\n",
    "    return Y, Z_true, W_true, b_true, P_true\n",
    "\n",
    "Y, Z_true, W_true, b_true, P_true = generate_synthetic()\n",
    "#print(\"Generated synthetic data Y:\", Y)\n",
    "print(\"Probabilities P_true:\", P_true)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95ff997d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[-1.71105412e+00, -1.06495274e+00, -1.09316925e+00,\n",
       "           2.62083581e+00],\n",
       "         [-7.00756428e-01, -2.30741267e-01,  6.00361284e-01,\n",
       "          -1.73657979e+00],\n",
       "         [ 1.87637300e+00, -1.39228406e-01, -3.10339330e+00,\n",
       "           8.17162042e+00],\n",
       "         ...,\n",
       "         [ 7.72232778e-01,  3.11617124e+00, -1.99447777e+00,\n",
       "          -3.19709408e-01],\n",
       "         [-1.06581143e-01, -2.63626899e+00, -6.36258432e+00,\n",
       "          -7.86990551e-01],\n",
       "         [-5.64027760e+00, -1.22205159e-01, -2.23867108e+00,\n",
       "          -2.48619272e+00]],\n",
       " \n",
       "        [[-6.59632427e+00,  2.11717433e+00, -8.15181360e+00,\n",
       "           2.20473665e+00],\n",
       "         [-5.02331281e-01,  2.14396987e+00,  2.74467777e+00,\n",
       "           6.11911946e+00],\n",
       "         [ 4.27501508e+00, -1.05397775e+00,  2.07603887e+00,\n",
       "           2.92050684e+00],\n",
       "         ...,\n",
       "         [ 2.04065420e-01,  2.90940798e+00,  1.59275718e-01,\n",
       "          -1.09404539e+00],\n",
       "         [ 1.36079035e-01, -2.94124868e+00, -6.91230112e-01,\n",
       "          -1.70934611e+00],\n",
       "         [ 9.55673613e-02,  3.52025438e+00, -7.11166035e-01,\n",
       "          -4.58798999e+00]],\n",
       " \n",
       "        [[-1.99727523e+00,  4.89719704e+00, -1.72161723e+00,\n",
       "          -4.86157816e+00],\n",
       "         [ 1.58500283e+00, -3.10754980e+00, -3.31618048e+00,\n",
       "           5.64858921e+00],\n",
       "         [ 1.46467751e+00, -4.27984949e+00, -5.44619538e+00,\n",
       "           1.10098009e+00],\n",
       "         ...,\n",
       "         [ 3.48988243e+00, -1.71777669e+00, -7.93145443e+00,\n",
       "           5.06182240e+00],\n",
       "         [ 2.77721871e-01,  3.79273986e+00,  3.53249590e+00,\n",
       "          -4.29379428e+00],\n",
       "         [ 9.09779108e-01, -1.67372977e+00, -2.26768643e+00,\n",
       "          -4.22365009e+00]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[ 4.05225381e+00,  6.36838740e-01,  2.39426605e+00,\n",
       "          -3.29546738e+00],\n",
       "         [-6.00459980e-01, -4.96599417e+00,  1.03076420e+00,\n",
       "           4.05777783e+00],\n",
       "         [ 1.86233127e+00, -2.14949794e-01,  7.24968168e-01,\n",
       "           2.76034068e-01],\n",
       "         ...,\n",
       "         [-3.34120163e+00,  5.51978473e+00, -2.29901541e+00,\n",
       "          -1.12973449e-01],\n",
       "         [-8.34835047e+00,  5.72531295e+00, -1.55587480e+00,\n",
       "          -1.26044863e+00],\n",
       "         [ 1.32721273e+00,  4.54923217e-03,  1.95547284e+00,\n",
       "           1.55417955e+00]],\n",
       " \n",
       "        [[-4.28912570e+00,  1.05670635e+00, -7.05700990e+00,\n",
       "           1.18114200e+00],\n",
       "         [-3.81553778e+00,  2.97586011e+00, -6.56364038e-01,\n",
       "          -1.34253694e+00],\n",
       "         [ 1.16798533e+00,  2.49114840e+00, -3.60459534e+00,\n",
       "          -4.00514875e+00],\n",
       "         ...,\n",
       "         [-1.91528419e+00,  2.90120324e+00, -1.08605667e+00,\n",
       "          -1.18654769e+00],\n",
       "         [-2.28896959e+00,  3.37745226e+00,  5.51772569e+00,\n",
       "          -6.10069775e-01],\n",
       "         [ 8.71724539e+00, -1.92291340e+00,  2.64199783e+00,\n",
       "          -1.00105039e+00]],\n",
       " \n",
       "        [[ 9.74471739e-01,  1.69595390e+00,  2.36220353e+00,\n",
       "           1.28394690e+00],\n",
       "         [ 7.59648650e-01, -4.03028868e+00,  4.17573081e+00,\n",
       "          -5.44752276e+00],\n",
       "         [ 1.30302978e+00,  2.78547425e-01, -1.19926698e+00,\n",
       "          -9.18657573e-01],\n",
       "         ...,\n",
       "         [-1.21875079e+00,  1.64761742e+00,  2.35226826e+00,\n",
       "          -6.51535431e+00],\n",
       "         [-4.25044392e+00,  9.16626219e-01, -4.18553440e+00,\n",
       "           1.45139306e+00],\n",
       "         [-8.44339886e-01,  1.30412614e+00,  4.62258459e+00,\n",
       "          -7.45959781e-01]]], shape=(500, 10, 4)),\n",
       " array([[-2.1025632 , -1.21650952, -2.32498151, -0.80350844],\n",
       "        [-2.22123331, -1.37429989, -2.01813202, -1.60420219],\n",
       "        [-1.92283409, -1.26245126, -2.28377828, -1.38559758],\n",
       "        ...,\n",
       "        [-1.65828841, -1.37871331, -2.05215382, -1.3475867 ],\n",
       "        [-1.62737626, -1.41463835, -1.97215262, -1.78562131],\n",
       "        [-1.57283394, -1.14525401, -1.62083769, -1.48460589]],\n",
       "       shape=(500, 4)),\n",
       " array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0., 0., 0., ..., 1., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [1., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]], shape=(500, 150, 10)))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Instantiate your sampler ----\n",
    "sampler = GibbsSamplerLLFM(\n",
    "    Data=Y,\n",
    "    K=10,              \n",
    "    alpha=0.1,\n",
    "    sigma_w=3.0,\n",
    "    mu_b=-1,\n",
    "    sigma_b=1,\n",
    "    #fixed_bias=[-0.5,-0.5,-0.5,-0.5],\n",
    "    n_iter=1000,\n",
    "    burn=200,\n",
    "    n_subsample=500\n",
    ")\n",
    "\n",
    "# ---- Run MCMC ----\n",
    "sampler.run()\n",
    "sampler.get_posterior_samples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41f54c4f-8e0a-4a39-aec3-3551544d652b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior grouping by number of active features\n",
      "\n",
      "Number of samples with zero active features: 329/500\n",
      "\n",
      "Group with 1 active features:\n",
      "  Number of posterior samples: 152/500\n",
      "  Average usage per feature (size-biased order):\n",
      "[23.04605263]\n",
      "  Average weights:\n",
      "[[-9.64667617 -6.77906104 -9.82822426 -7.38930992]]\n",
      "  Average Bias:\n",
      "[-1.73822439 -0.96459792 -1.75646363 -1.13505011]\n",
      "--------------------------------------------------\n",
      "Group with 2 active features:\n",
      "  Number of posterior samples: 16/500\n",
      "  Average usage per feature (size-biased order):\n",
      "[17.375   9.5625]\n",
      "  Average weights:\n",
      "[[-7.38958803 -6.53161199 -8.62997512 -6.35982414]\n",
      " [-5.50005081 -4.46646521 -6.41673919 -3.07870014]]\n",
      "  Average Bias:\n",
      "[-1.6518161  -0.91043558 -1.63861712 -1.01216274]\n",
      "--------------------------------------------------\n",
      "Group with 3 active features:\n",
      "  Number of posterior samples: 3/500\n",
      "  Average usage per feature (size-biased order):\n",
      "[26.66666667 17.33333333  9.33333333]\n",
      "  Average weights:\n",
      "[[ -9.15754325  -7.90706651 -12.41989785  -8.06234849]\n",
      " [ -4.82866466  -5.8042368   -6.11647057  -6.41958235]\n",
      " [ -5.12919588  -4.05139097  -6.68678404  -4.01318319]]\n",
      "  Average Bias:\n",
      "[-1.31127491 -0.60528813 -1.69400811 -0.84455189]\n",
      "--------------------------------------------------\n",
      "log_numerator: 2.2019148774405886\n",
      "log_denom: 3.7169786685540385\n",
      "P(pred=1 | conds=[1,0,0]): 0.21979416348612876\n"
     ]
    }
   ],
   "source": [
    "latent_features( Z_post=sampler.good_samples_Z, W_post=sampler.good_samples_W, b_post=sampler.good_samples_b)\n",
    "p1given0 = sampler.posterior_predictive([1, 0, 0])\n",
    "print(\"P(pred=1 | conds=[1,0,0]):\", p1given0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f22f1786-c0b3-4547-8f90-92b6d3afa17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_true_parameters_to_file(\n",
    "    filename,\n",
    "    Y,\n",
    "    Z_true,\n",
    "    W_true,\n",
    "    b_true\n",
    "):\n",
    "    \"\"\"\n",
    "    Writes a structured summary of the true synthetic parameters to file.\n",
    "    Appends to file so posterior summaries can be added later.\n",
    "    \"\"\"\n",
    "\n",
    "    T, S = Y.shape\n",
    "    K_true = Z_true.shape[1]\n",
    "\n",
    "    with open(filename, \"a\") as f:\n",
    "\n",
    "        f.write(\"=\" * 60 + \"\\n\")\n",
    "        f.write(\"TRUE SYNTHETIC PARAMETERS\\n\")\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "        f.write(f\"T (observations): {T}\\n\")\n",
    "        f.write(f\"S (dimensions):   {S}\\n\")\n",
    "        f.write(f\"K_true (features): {K_true}\\n\\n\")\n",
    "\n",
    "        # ---- Latent feature usage ----\n",
    "        usage = Z_true.sum(axis=0)\n",
    "\n",
    "        f.write(\"True latent feature usage counts:\\n\")\n",
    "        for k in range(K_true):\n",
    "            f.write(f\"  Feature {k}: active {usage[k]} times\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # ---- True weights ----\n",
    "        f.write(\"True weight matrix W_true (K x S):\\n\")\n",
    "        f.write(np.array2string(W_true, precision=3))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        # ---- True bias ----\n",
    "        f.write(\"True bias vector b_true:\\n\")\n",
    "        f.write(np.array2string(b_true, precision=3))\n",
    "        f.write(\"\\n\\n\")\n",
    "\n",
    "        # ---- Observed data summary ----\n",
    "        f.write(\"Observed data summary (Y):\\n\")\n",
    "        f.write(f\"  Mean activation per dimension: {Y.mean(axis=0)}\\n\")\n",
    "        f.write(f\"  Overall mean activation: {Y.mean():.4f}\\n\\n\")\n",
    "\n",
    "        f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "\n",
    "write_true_parameters_to_file('validation1_overlapping_feature_weight.txt', Y, Z_true, W_true, b_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e07022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_numerator: 0.3267140295466753\n",
      "log_denom: 2.3261272741329133\n",
      "P(pred=1 | conds=[1,0,0]): 0.13541471524802723\n"
     ]
    }
   ],
   "source": [
    "from evals import latent_features_to_file\n",
    "latent_features_to_file(filename='validation1_overlapping_feature_weight.txt', Z_post=sampler.good_samples_Z, W_post=sampler.good_samples_W, b_post=sampler.good_samples_b)\n",
    "p1given0 = sampler.posterior_predictive([1, 0, 0])\n",
    "print(\"P(pred=1 | conds=[1,0,0]):\", p1given0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426d9d3b-2add-4b73-b95d-17ada5bd4dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (rotenv)",
   "language": "python",
   "name": "rotenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
