{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ff2334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from models import GibbsSamplerLLFM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eff59ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated synthetic data Y with shape: (150, 3)\n",
      "[[1 1 0]\n",
      " [1 1 0]\n",
      " [1 0 1]\n",
      " [1 1 0]\n",
      " [1 0 0]\n",
      " [1 1 0]\n",
      " [1 1 1]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 0 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 0 0]\n",
      " [0 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 0 0]\n",
      " [1 1 0]\n",
      " [1 0 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 0 0]\n",
      " [1 1 1]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [0 1 1]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 1]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]\n",
      " [1 0 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 0 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [0 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 1 0]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [1 0 0]\n",
      " [1 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 0 0]\n",
      " [0 1 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 1 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 0]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 1 1]\n",
      " [0 0 1]\n",
      " [1 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [0 0 1]\n",
      " [1 1 0]]\n",
      "[[0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.04742587]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]\n",
      " [0.04742587 0.04742587 0.88079708]]\n"
     ]
    }
   ],
   "source": [
    "def generate_synthetic(T=150, S=3, K_true=2, seed=0):\n",
    "\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # ----- True latent features -----\n",
    "    Z_true = np.zeros((T, K_true))\n",
    "    Z_true[:75, 0] = 1\n",
    "    Z_true[100:, 1] = 1\n",
    "\n",
    "    # ----- True weights -----\n",
    "    W_true = np.zeros((K_true, S))\n",
    "    W_true[0,0] = 5\n",
    "    W_true[0,1] = 5\n",
    "    W_true[1,2] = 5\n",
    "\n",
    "\n",
    "    # ----- True bias -----\n",
    "    b_true = np.array([-3, -3, -3])\n",
    "\n",
    "    # ----- Generate observations -----\n",
    "    logits = Z_true @ W_true + b_true\n",
    "    P_true = expit(logits)\n",
    "    Y = np.random.binomial(1, P_true)\n",
    "\n",
    "    return Y, Z_true, W_true, b_true, P_true\n",
    "\n",
    "Y, Z_true, W_true, b_true, P_true = generate_synthetic()\n",
    "print(\"Generated synthetic data Y with shape:\", Y.shape)\n",
    "print(Y)\n",
    "print(P_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95ff997d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Instantiate your sampler ----\n",
    "sampler = GibbsSamplerLLFM(\n",
    "    Data=Y,\n",
    "    K=10,              \n",
    "    alpha=0.5,\n",
    "    sigma_w=2.0,\n",
    "    sigma_b=0.5,\n",
    "    mu_b=-5.0,\n",
    "    n_iter=5000,\n",
    "    burn=1000,\n",
    "    n_subsample=1000\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5e07022",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of active features across iterations: 3.37\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ---- Run MCMC ----\n",
    "sampler.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ed045cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Posterior means ----\n",
    "W_post, b_post, Z_post = sampler.get_posterior_samples()\n",
    "W_post = np.array(W_post)\n",
    "b_post = np.array(b_post)\n",
    "Z_post = np.array(Z_post)\n",
    "\n",
    "feature_counts = np.array([Z.sum(axis=0) for Z in Z_post])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad4d1f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior grouping by number of active features\n",
      "\n",
      "Number of samples with zero active features: 0\n",
      "\n",
      "Group with 1 active features:\n",
      "  Number of posterior samples: 89\n",
      "  Average weights:\n",
      "[[ 1.4680417   1.11306289 -8.56376648]]\n",
      "  Average bias:\n",
      "[-1.05245868 -1.03822733 -0.48463459]\n",
      "----------------------------------------\n",
      "Group with 2 active features:\n",
      "  Number of posterior samples: 569\n",
      "  Average weights:\n",
      "[[ 1.72470286  1.32898924 -9.73405149]\n",
      " [-6.36253243 -6.61372742  1.52567962]]\n",
      "  Average bias:\n",
      "[-0.83414414 -0.88582812 -0.777894  ]\n",
      "----------------------------------------\n",
      "Group with 3 active features:\n",
      "  Number of posterior samples: 278\n",
      "  Average weights:\n",
      "[[ 1.61164627  1.21580596 -8.28666287]\n",
      " [-3.66337654 -3.71033979 -0.26241187]\n",
      " [-2.80426502 -2.91136189 -0.10937644]]\n",
      "  Average bias:\n",
      "[-0.84488258 -0.83572993 -0.81853107]\n",
      "----------------------------------------\n",
      "Group with 4 active features:\n",
      "  Number of posterior samples: 57\n",
      "  Average weights:\n",
      "[[ 1.33724584  0.93705926 -6.61300986]\n",
      " [-2.83173751 -2.58077518 -0.56910063]\n",
      " [-2.6142469  -2.52300862 -0.06525699]\n",
      " [-0.84315295 -1.01080172 -1.26577878]]\n",
      "  Average bias:\n",
      "[-0.73034567 -0.81039171 -0.93113548]\n",
      "----------------------------------------\n",
      "Group with 5 active features:\n",
      "  Number of posterior samples: 7\n",
      "  Average weights:\n",
      "[[-1.15058002 -0.87899125 -3.2605811 ]\n",
      " [-4.18796697 -5.16936722 -0.12330325]\n",
      " [ 1.18654632  1.30734098 -3.98718052]\n",
      " [ 1.20489924  0.42625619 -2.41217058]\n",
      " [ 0.96447852  1.62189913 -1.60994112]]\n",
      "  Average bias:\n",
      "[-0.73261415 -0.82718585 -1.07633964]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# ---- Get posterior samples ----\n",
    "W_post, b_post, Z_post = sampler.get_posterior_samples()\n",
    "Z_post = np.array(Z_post)  # shape: (n_samples, T, K)\n",
    "W_post = np.array(W_post)  # shape: (n_samples, K, S)\n",
    "b_post = np.array(b_post)  # shape: (n_samples, S)\n",
    "\n",
    "n_samples, T, K = Z_post.shape\n",
    "S = W_post.shape[2]\n",
    "\n",
    "threshold = 12\n",
    "\n",
    "# Normal dictionary\n",
    "groups = {}\n",
    "zeros = 0\n",
    "for it in range(n_samples):\n",
    "    Z = Z_post[it]   # (T, K)\n",
    "    W = W_post[it]   # (K, S)\n",
    "    b = b_post[it]\n",
    "\n",
    "    usage = Z.sum(axis=0)\n",
    "\n",
    "    # Identify active features\n",
    "    active_idx = np.where(usage > threshold)[0]\n",
    "    k_active = len(active_idx)\n",
    "\n",
    "    if k_active == 0:\n",
    "\n",
    "        zeros += 1\n",
    "        continue  # Skip samples with no active features\n",
    "\n",
    "    # Sort active features by usage descending\n",
    "    sorted_idx = active_idx[np.argsort(usage[active_idx])[::-1]]\n",
    "\n",
    "    W_perm = W[sorted_idx, :]\n",
    "\n",
    "    # ---- Manually create group if needed ----\n",
    "    if k_active not in groups:\n",
    "        groups[k_active] = []\n",
    "\n",
    "    groups[k_active].append((W_perm, b))\n",
    "\n",
    "\n",
    "# ---- Report results ----\n",
    "print(\"Posterior grouping by number of active features\\n\")\n",
    "print(f\"Number of samples with zero active features: {zeros}\\n\")\n",
    "for k in sorted(groups.keys()):\n",
    "    samples = groups[k]\n",
    "    n_group = len(samples)\n",
    "\n",
    "    print(f\"Group with {k} active features:\")\n",
    "    print(f\"  Number of posterior samples: {n_group}\")\n",
    "\n",
    "    W_stack = np.array([w for w, _ in samples])   # (n_group, k, S)\n",
    "    b_stack = np.array([b for _, b in samples])   # (n_group, S)\n",
    "\n",
    "    W_mean = W_stack.mean(axis=0)\n",
    "    b_mean = b_stack.mean(axis=0)\n",
    "\n",
    "    print(\"  Average weights:\")\n",
    "    print(W_mean)\n",
    "\n",
    "    print(\"  Average bias:\")\n",
    "    print(b_mean)\n",
    "    print(\"-\" * 40)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea46b836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posterior mean probabilities shape: (200, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "\n",
    "# Convert lists of posterior samples to arrays\n",
    "# samples_Z: list of (T, K), samples_W: list of (K, S), samples_b: list of (S,)\n",
    "Z_array = np.array(Z_post)  # shape: (n_samples, T, K)\n",
    "W_array = np.array(W_post)  # shape: (n_samples, K, S)\n",
    "b_array = np.array(b_post)  # shape: (n_samples, S)\n",
    "\n",
    "# Compute linear predictor for each posterior sample\n",
    "# Shape: (n_samples, T, S)\n",
    "eta_array = np.einsum('nTk,nKs->nTs', Z_array, W_array) + b_array[:, None, :]\n",
    "\n",
    "# Apply sigmoid to get probabilities\n",
    "P_post_array = expit(eta_array)\n",
    "\n",
    "# Average over posterior samples to get posterior mean probability\n",
    "P_post_mean = np.mean(P_post_array, axis=0)  # shape: (T, S)\n",
    "\n",
    "print(\"Posterior mean probabilities shape:\", P_post_mean.shape)\n",
    "# P_post_mean[t, s] is the posterior mean probability of y[t, s] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bee147ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 5: [[0.88079708 0.04742587 0.04742587 0.88079708]\n",
      " [0.88079708 0.04742587 0.04742587 0.88079708]\n",
      " [0.88079708 0.04742587 0.04742587 0.88079708]\n",
      " [0.88079708 0.04742587 0.04742587 0.88079708]\n",
      " [0.88079708 0.04742587 0.04742587 0.88079708]]\n",
      "last 5: [[0.88079708 0.88079708 0.04742587 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587 0.04742587]\n",
      " [0.88079708 0.88079708 0.04742587 0.04742587]]\n",
      "P_post_mean first 5: [[0.50503783 0.12527392 0.06526823 0.37663449]\n",
      " [0.46225779 0.16668754 0.09222597 0.35412327]\n",
      " [0.46349478 0.17512705 0.10080571 0.36984361]\n",
      " [0.50250758 0.12687998 0.06586019 0.36649709]\n",
      " [0.50178207 0.12313375 0.06569088 0.38537296]]\n",
      "P_post_mean last 5: [[0.49057412 0.16951726 0.08479603 0.33976924]\n",
      " [0.48832985 0.17518506 0.08605647 0.33762389]\n",
      " [0.48817762 0.17487245 0.08951938 0.33856437]\n",
      " [0.41546831 0.22170545 0.13212784 0.34772555]\n",
      " [0.49502107 0.1704351  0.08669194 0.338816  ]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(f'first 5: {P_true[:5]}')\n",
    "print(f'last 5: {P_true[-5:]}')\n",
    "print(f'P_post_mean first 5: {P_post_mean[:5]}')\n",
    "print(f'P_post_mean last 5: {P_post_mean[-5:]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compvenv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
